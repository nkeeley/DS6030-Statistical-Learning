---
title: "Homework #1: Supervised Learning" 
author: "**Nicholas Keeley**"
date: "Due: Tue Sept 07 | 2:55pm"
output: R6030::homework
---

**DS 6030 | Fall 2021 | University of Virginia**

Note: On my honor, I have not given or received (uncredited) aid on this assessment. This code represents my own work. /s Nicholas Keeley 

Credits: Drew Pearson (compared density plots, and he showed me the geom_density() function)
------------------------------------------------------------------------

```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6030")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```

# Required R packages and Directories

::: {.solution}
```{r packages, message=FALSE, warning=FALSE, echo=FALSE}
data.dir = 'https://mdporter.github.io/DS6030/data/' # data directory
library(R6030)     # functions for DS-6030
library(tidyverse) # functions for data manipulation    
library("ISLR")
library("moderndive")
library("rpart")
library("rpart.plot")
library("randomForest")
library("boot")
library("caret")
library("broom")
library("glmnet")
library("FNN")
library("modelr")
library("tidymodels")
library("e1071")
library("MASS")
library("fitdistrplus")
library("ks")
library("mclust")
library("mixtools")
library("gbm")
library("xgboost")
library("arules")
library("igraph")
```
:::

# Problem 1: Evaluating a Regression Model 

## a. Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{N}(0, 1) \\
Y &= -1 + .5X + .2X^2 + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma)
\end{align*}


::: {.solution}

```{r problem1_code, echo=TRUE, results="hide"}
func1=function(n,sigma){
  epsilon= rnorm(n,0,sigma)
  data=tibble(X=rnorm(n,0,1),
              Y=-1+0.5*X+.2*(X^2)+epsilon,
              epsilon)
}
data<-func1(200,2)
data
```


:::

## b. Simulate $n=100$ realizations from these distributions using $\sigma=3$. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. 
- Use `set.seed(611)` prior to generating the data.


::: {.solution}

```{r problem1_b_code, echo=FALSE, results="hide"}

## Randomize and generate the data

set.seed(611)
data2=func1(n=100,sigma=3)
data2

## True population

tr_pop=tibble(X=data2$X,
              Y=-1+0.5*X+.2*(X^2))
tr_pop



```

```{r problem1_b_visual, echo=TRUE}

ggplot(aes(X, Y), data=data2) + 
geom_point() +
geom_smooth(aes(y=tr_pop$Y), se=FALSE, color="black")
  
```


:::



## c. Fit three polynomial regression models using least squares: linear, quadratic, and cubic. Produce another scatterplot, add the fitted lines and true population line $f(x)$  using different colors, and add a legend that maps the line color to a model.
- Note: The true model is quadratic, but we are also fitting linear (less complex) and cubic (more complex) models. 


::: {.solution}

```{r problem1_c_code, echo=FALSE, results="hide"}

## Helper code

helper_x=function(input_length){
  xseq=seq(-3,3,length=input_length)
  xeval=tibble(X=xseq)
}

## Linear regression

lm1=lm(Y~X,data=data2)
tidy(lm1)

## Quadratic

m2=lm(Y~poly(X, degree=2), data=data2)
tidy(m2)
pred2=predict(m2, newdata=helper_x(100))
pred2
data2$pred2=pred2

## Cubic

m3=lm(Y~poly(X, degree=3), data=data2)
tidy(m3)
pred3=predict(m3, newdata=helper_x(100))
pred3
data2$pred3=pred3
```
```{r problem1_c_visual, echo=TRUE}

data2 %>%
  ggplot(aes(X, Y)) + 
  geom_point() +
  geom_smooth(aes(y=tr_pop$Y), se=FALSE, color="purple") +
  geom_smooth(method="lm",se=FALSE,color="blue") +
  geom_smooth(aes(y=pred2), se=FALSE, color = "green") +
  geom_smooth(aes(y=pred3), se=FALSE, color = "red")
  

  
```


:::


## d. Simulate a *test data* set of 10,000 observations from the same distributions. Use `set.seed(612)` prior to generating the test data.   
- Calculate the estimated mean squared error (MSE) for each model. 
- Are the results as expected? 

::: {.solution}

My MSEs were as follows: linear model = 9.22, quadratic model = 9.17, cubic model = 9.28.
I generally expected these results due to bias/variance tradeoff with increasing model complexity. The MSE of the linear model is likely higher because it has too much bias. The MSE of the quadratic model goes down as expected with the introduction of more variables, and the MSE of the cubic model spikes up past the linear model, likely due to higher variance. Also, the original data was generated using a quadratic model, so a quadratic model should have the lowest MSE.

```{r problem1_d_code, echo=FALSE, results="hide"}

## Seed and generate data

set.seed(612)
data3=func1(n=10000,sigma=3)

pred1=predict(lm1, newdata=data3)
pred1
data3$pred1=pred1
pred2=predict(m2, newdata=data3)
pred2
data3$pred2=pred2
pred3=predict(m3, newdata=data3)
pred3
data3$pred3=pred3

```

```{r problem1_d_visual}
## MSE for each function

mse1=mean((data3$Y-data3$pred1)^2)
mse2=mean((data3$Y-data3$pred2)^2)
mse3=mean((data3$Y-data3$pred3)^2)

## Tibble for data

df=tibble(mse1,mse2,mse3)
df

## True population

tr_pop=tibble(X=data3$X,
              Y=-1+0.5*X+.2*(X^2))
tr_pop

data3 %>%
  ggplot(aes(X, Y)) + 
  geom_point() +
  geom_smooth(aes(y=tr_pop$Y), se=FALSE, color="purple") +
  geom_smooth(method="lm",se=FALSE,color="blue") +
  geom_smooth(aes(y=pred2), se=FALSE, color = "green") +
  geom_smooth(aes(y=pred3), se=FALSE, color = "red")


```

:::



## e. What is the best achievable MSE? That is, what is the MSE if the true $f(x)$ was used to evaluate the test set? How close does the best method come to achieving the optimum? 


::: {.solution}

Because the data was generated from a gaussian distribution with a standard deviation of 3, the best achievable MSE is 9 (3*3), since this represents the irreducible error. The quadratic model achieved an MSE of 9.172, which is very close.

:::


## f. The MSE scores obtained in part *d* came from one realization of training data. Here will we explore how much variation there is in the MSE scores by replicating the simulation many times. 

- Re-run parts b. and c. (i.e., generate training data and fit models) 100 times.
- Calculate the MSE for all simulations. 
- Create kernel density or histogram plots of the resulting MSE values for each model. 
- Use `set.seed(613)` prior to running the simulation and do not set the seed in any other places.
- Use the same test data from part d. (This question is only about the variability that comes from the training data). 

::: {.solution}

```{r problem1_f_code, echo=FALSE, results="hide"}
## Setup

set.seed(613)

## Create output variable and clean test dataset (without old predictions)

df_output_lin=tibble(mse = vector("double",100))
df_output_quad=tibble(mse = vector("double",100))
df_output_cub=tibble(mse = vector("double",100))

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Linear 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=100,sigma=3) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_lin[i,1]=mse_i # Store output into cell
}

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Quadratic 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=100,sigma=3) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_quad[i,1]=mse_i # Store output into cell
}

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Cubic 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=100,sigma=3) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_cub[i,1]=mse_i # Store output into cell
}

## Stitch

df_output_lin$label="1"
df_output_quad$label="2"
df_output_cub$label="3"

new = rbind(df_output_lin, df_output_quad)
new=rbind(new,df_output_cub)

```


```{r }

## Visuals

ggplot(data = new, aes(mse,color=label)) +
  geom_density(position="identity") +
  scale_color_manual(labels = c("Linear", "Quadratic","Cubic"),values=c("Blue","Green", "Red")) 

```



:::

## g. Show a count of how many times each model was the best. That is, out of the 100 simulations, count how many times each model had the lowest MSE.

::: {.solution}



```{r, echo=FALSE}

## Rebind

df_output_lin$mse_lin=df_output_lin$mse
df_output_quad$mse_quad=df_output_quad$mse
df_output_cub$mse_cub=df_output_cub$mse

new2 = cbind(df_output_lin, df_output_quad)
new2
new2=cbind(new2,df_output_cub)

## Set winner row

new2$winner_lin=ifelse(new2$mse_lin<new2$mse_cub & new2$mse_lin<new2$mse_quad, 1,0)
new2$winner_quad=ifelse(new2$mse_quad<new2$mse_cub & new2$mse_quad<new2$mse_lin, 1,0)
new2$winner_cub=ifelse(new2$mse_cub<new2$mse_lin & new2$mse_cub<new2$mse_quad, 1,0)

## Create grouped df

lin_wins = sum(new2$winner_lin)
quad_wins = sum(new2$winner_quad)
cub_wins = sum(new2$winner_cub)
grouped_df1=tibble()
grouped_df1$lin_wins = lin_wins
grouped_df1$quad_wins = quad_wins
grouped_df1$cub_wins = cub_wins
grouped_df1[1,1]= lin_wins
grouped_df1[1,2] = quad_wins
grouped_df1[1,3] = cub_wins

grouped_df1
```

:::


## h. Repeat the simulation in part *f*, but use $\sigma=2$. Report the number of times each model was best (you do not need to produce any plots). Use the same `set.seed(613)` prior to running the simulation and do not set the seed in any other places.


::: {.solution}

```{r echo=FALSE, results="hide"}


set.seed(613)

## Create output variable and clean test dataset (without old predictions)

df_output_lin=tibble(mse = vector("double",100))
df_output_quad=tibble(mse = vector("double",100))
df_output_cub=tibble(mse = vector("double",100))

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Linear 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=100,sigma=2) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_lin[i,1]=mse_i # Store output into cell
}

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Quadratic 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=100,sigma=2) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_quad[i,1]=mse_i # Store output into cell
}

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Cubic 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=100,sigma=2) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_cub[i,1]=mse_i # Store output into cell
}

## Stitch

df_output_lin$label="1"
df_output_quad$label="2"
df_output_cub$label="3"

new = rbind(df_output_lin, df_output_quad)
new=rbind(new,df_output_cub)

## Rebind

df_output_lin$mse_lin=df_output_lin$mse
df_output_quad$mse_quad=df_output_quad$mse
df_output_cub$mse_cub=df_output_cub$mse

new2 = cbind(df_output_lin, df_output_quad)
new2
new2=cbind(new2,df_output_cub)


## Set winner row

new2$winner_lin=ifelse(new2$mse_lin<new2$mse_cub & new2$mse_lin<new2$mse_quad, 1,0)
new2$winner_quad=ifelse(new2$mse_quad<new2$mse_cub & new2$mse_quad<new2$mse_lin, 1,0)
new2$winner_cub=ifelse(new2$mse_cub<new2$mse_lin & new2$mse_cub<new2$mse_quad, 1,0)

## Create grouped df

lin_wins = sum(new2$winner_lin)
quad_wins = sum(new2$winner_quad)
cub_wins = sum(new2$winner_cub)
grouped_df1=tibble()
grouped_df1$lin_wins = lin_wins
grouped_df1$quad_wins = quad_wins
grouped_df1$cub_wins = cub_wins
grouped_df1[1,1]= lin_wins
grouped_df1[1,2] = quad_wins
grouped_df1[1,3] = cub_wins
```
```{r}
grouped_df1
```



:::



## i. Repeat *h*, but now use $\sigma=4$ and $n=300$. 

::: {.solution}


```{r echo=FALSE, results="hide"}

set.seed(613)

## Create output variable and clean test dataset (without old predictions)

df_output_lin=tibble(mse = vector("double",100))
df_output_quad=tibble(mse = vector("double",100))
df_output_cub=tibble(mse = vector("double",100))

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Linear 100 realization MSEs

for(i in 1:100){
  data_i=func1(n=300,sigma=4) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_lin[i,1]=mse_i # Store output into cell
}

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Quadratic 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=300,sigma=4) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_quad[i,1]=mse_i # Store output into cell
}

## Clean test data

data4 = data3[c("X","Y","epsilon")]

## Cubic 200 realization MSEs

for(i in 1:100){
  data_i=func1(n=300,sigma=4) # Generate this random realization of training data
  lm_i=lm(Y~X,data=data_i) # Fit linear model to this realization
  pred_i=predict(lm_i, newdata=data4) # Predict using 10K test data 
  data4$pred_i=pred_i # Append prediction to test data set
  mse_i=mean((data4$Y-data4$pred_i)^2) # Calculate this realization MSE
  df_output_cub[i,1]=mse_i # Store output into cell
}

## Stitch

df_output_lin$label="1"
df_output_quad$label="2"
df_output_cub$label="3"

new = rbind(df_output_lin, df_output_quad)
new=rbind(new,df_output_cub)

## Rebind

df_output_lin$mse_lin=df_output_lin$mse
df_output_quad$mse_quad=df_output_quad$mse
df_output_cub$mse_cub=df_output_cub$mse

new2 = cbind(df_output_lin, df_output_quad)
new2
new2=cbind(new2,df_output_cub)

## Set winner row

new2$winner_lin=ifelse(new2$mse_lin<new2$mse_cub & new2$mse_lin<new2$mse_quad, 1,0)
new2$winner_quad=ifelse(new2$mse_quad<new2$mse_cub & new2$mse_quad<new2$mse_lin, 1,0)
new2$winner_cub=ifelse(new2$mse_cub<new2$mse_lin & new2$mse_cub<new2$mse_quad, 1,0)

## Create grouped df

lin_wins = sum(new2$winner_lin)
quad_wins = sum(new2$winner_quad)
cub_wins = sum(new2$winner_cub)
grouped_df1=tibble()
grouped_df1$lin_wins = lin_wins
grouped_df1$quad_wins = quad_wins
grouped_df1$cub_wins = cub_wins
grouped_df1[1,1]= lin_wins
grouped_df1[1,2] = quad_wins
grouped_df1[1,3] = cub_wins



```
```{r}
grouped_df1
```

:::

## j. Describe the effects $\sigma$ and $n$ has on selection of the best model? Why is the *true* model form (i.e., quadratic) not always the *best* model to use when prediction is the goal? 

::: {.solution}

Increasing sigma changes the dispersion of the training data, which introduces variance. This lowers the predictive accuracy of high complexity models especially, due to overfitting. Increasing sample size of the training data reduced the variance within more complex models, but this might not always be true. At the end of the day, true model might not always be the best model to use when prediction is the goal because we don't care about the traininig data fit - we care about how low the MSE is on the test data. If the parameters of the test data are different, we can result in high training-fit models that are sub-optimal for predictive purposes, and vice versa.

:::




