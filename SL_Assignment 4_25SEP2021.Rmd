---
title: "Homework #4: Classification" 
author: "**Nick Keeley**"
date: "Due: Tue Sept 28 | 3:25pm"
output: R6030::homework
---

**DS 6030 | Fall 2021 | University of Virginia**

------------------------------------------------------------------------

```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6030")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```




# Required R packages and Directories

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/DS6030/data/' # data directory
library(R6030)     # functions for SYS-6030
library(tidyverse) # functions for data manipulation
library(glmnet)
library(skimr)
library(ggplot2)
```
:::


# Crime Linkage

Crime linkage attempts to determine if two or more unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

- `spatial` is the spatial distance between the crimes
- `temporal` is the fractional time (in days) between the crimes
- `tod` and `dow` are the differences in time of day and day of week between the crimes
- `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
- `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
- The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).


These problems use the [linkage-train](https://mdporter.github.io/SYS6018/data/linkage_train.csv) and [linkage-test](https://mdporter.github.io/SYS6018/data/linkage_test.csv) datasets (click on links for data). 



# Problem 1: Penalized Regression for Crime Linkage

## a. Fit a penalized *linear regression* model to predict linkage. Use a lasso, ridge, or elasticnet penalty (your choice). 
- Report the value of $\alpha$ used (if elasticnet)
- Report the value of $\lambda$ used
- Report the estimated coefficients


::: {.solution}


```{r}

## Read in the data and examine

df.train=read_csv("https://mdporter.github.io/SYS6018/data/linkage_train.csv")
df.test=read_csv("https://mdporter.github.io/SYS6018/data/linkage_test.csv")
#skim(df.test)
#ggplot(df.train, aes(x=df.train$temporal, y=df.train$y)) + 
#  geom_violin()

## Set aside 10% of training data as holdout set

set.seed(2021)
n.train=length(df.train$y)
h=sample(1:n.train,(.1*n.train))
df.holdout=df.train[h,]
df.train2=slice(df.train,-h)
```


```{r}

## Setup for linear regression model with lasso penalty

X=glmnet::makeX(select(df.train2,-y),select(df.holdout,-y))
train.x=X$x
train.y=df.train2$y
test.x=X$xtest

a=1 # alpha
K=10 # number of folds
folds=rep(1:K, length=nrow(train.x)) %>% sample()

## Cross validation to fit penalized linear regression model

fit.linear=cv.glmnet(train.x, train.y, alpha=a)
tidy(fit.linear)



```
```{r echo=FALSE}

## Print coefficients

coef(fit.linear, s = "lambda.min")
```

```{r echo=FALSE}

## Print lambda minimum used

lambda.min=fit.linear$lambda.min
paste("Lambda: ", lambda.min, sep="")
```
```{r echo=FALSE}

## Print alpha

paste("Alpha: ", a,sep="")
```

:::



## b. Fit a penalized *logistic regression* model to predict linkage. Use a lasso, ridge, or elasticnet penalty (your choice).  
- Report the value of $\alpha$ used (if elasticnet)
- Report the value of $\lambda$ used
- Report the estimated coefficients

::: {.solution}
```{r}

## Cross validation to fit penalized logistic regression model

fit.log=cv.glmnet(train.x, train.y, alpha=a, family="binomial")
tidy(fit.log)
```
```{r echo=FALSE}

## Print coefficients

coef(fit.log, s = "lambda.min")
```

```{r echo=FALSE}

## Print lambda minimum used

lambda.min=fit.log$lambda.min
paste("Lambda: ", lambda.min, sep="")
```
```{r echo=FALSE}

## Print alpha

paste("Alpha: ", a,sep="")

```

## c. Produce one plot that has the ROC curves, using the *training data*, for both models (from part a and b). Use color and/or linetype to distinguish between models and include a legend.       

::: {.solution}

```{r}

#### Confusion matrix for linear model


## Get p(x) at each point for test data

p.hat = predict(fit.linear, test.x, type="response", s="lambda.min")

## Hard classification p(x) >= 5%, linkage = TRUE

G.hat=ifelse(p.hat >= .05,1,0)

## Confusion matrix

G.test=df.holdout$y
table(predicted=G.hat, truth = G.test) %>% addmargins()


## Performance data

performance.lin = tibble(truth = G.test, p.hat) %>%
  group_by(p.hat) %>% 
  summarize(n=n(),n.1=sum(truth),n.0=n-sum(truth)) %>% ungroup() %>%
  arrange(p.hat) %>%
  mutate(FN = cumsum(n.1), # false negatives
         TN = cumsum(n.0), # true negatives
         TP = sum(n.1) - FN, # true positives
         FP = sum(n.0) - TN, # false positives
         N = cumsum(n),
         TPR = TP/sum(n.1), FPR = FP/sum(n.0))
# number of cases predicted to be 1 TPR = TP/sum(n.1), FPR = FP/sum(n.0)) %>%


```

```{r}

#### Confusion matrix for LOG model


## Get p(x) at each point for test data

p.hat = predict(fit.log, test.x, type="response", s="lambda.min")

## Hard classification p(x) >= 5%, linkage = TRUE

G.hat=ifelse(p.hat >= .05,1,0)

## Confusion matrix

G.test=df.holdout$y
table(predicted=G.hat, truth = G.test) %>% addmargins()


## Performance data

performance.log = tibble(truth = G.test, p.hat) %>%
  group_by(p.hat) %>% 
  summarize(n=n(),n.1=sum(truth),n.0=n-sum(truth)) %>% ungroup() %>%
  arrange(p.hat) %>%
  mutate(FN = cumsum(n.1), # false negatives
         TN = cumsum(n.0), # true negatives
         TP = sum(n.1) - FN, # true positives
         FP = sum(n.0) - TN, # false positives
         N = cumsum(n),
         TPR = TP/sum(n.1), FPR = FP/sum(n.0))
# number of cases predicted to be 1 TPR = TP/sum(n.1), FPR = FP/sum(n.0)) %>%


```

```{r echo=FALSE}

library(yardstick)

performance.log %>%
  ggplot(aes(FPR, TPR)) + geom_path(color="blue") +
  geom_path(data=performance.lin, color="red") +
  geom_abline(lty=3) +
  coord_equal()
```




:::




## d. Recreate the ROC curve from the penalized logistic regression model using repeated hold-out data. The following steps will guide you:
- Fix $\alpha=.75$ 
- Run the following steps 25 times:
i. Hold out 500 observations
ii. Use the remaining observations to estimate $\lambda$ using 10-fold CV
iii. Predict the probability of linkage for the 500 hold-out observations
iv. Store the predictions and hold-out labels
- Combine the results and produce the hold-out based ROC curve
- Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter. 
    
::: {.solution}

```{r eval=TRUE}

## Setup

a=.75
M=25
grande=tibble()
K=10 # number of folds
folds=rep(1:K, length=nrow(train.x)) %>% sample()

#### Loop

for(M in 1:M){
  ## 500 holdout
  
  n.train=length(df.train$y)
  h=sample(1:n.train,(500))
  df.holdout=df.train[h,]
  df.train2=slice(df.train,-h)
  
  ## Setup for linear regression model with lasso penalty
  
  X=glmnet::makeX(select(df.train2,-y),select(df.holdout,-y))
  train.x=X$x
  train.y=df.train2$y
  test.x=X$xtest
  
  ## Cross validation to estimate lambda (min)
  

  fit.log2=cv.glmnet(train.x, train.y, alpha=a, foldid=folds, family="binomial")
  
  ## Make predictions
  
  p.hat = predict(fit.log2, test.x, type="response", s="lambda.min")
  
  ## Performance data
  
  G.test=df.holdout$y
  performance.log2 = tibble(truth = G.test, p.hat) %>%
    group_by(p.hat) %>% 
    summarize(n=n(),n.1=sum(truth),n.0=n-sum(truth)) %>% ungroup() %>%
    arrange(p.hat) %>%
    mutate(FN = cumsum(n.1), # false negatives
           TN = cumsum(n.0), # true negatives
           TP = sum(n.1) - FN, # true positives
           FP = sum(n.0) - TN, # false positives
           N = cumsum(n),
           TPR = TP/sum(n.1), FPR = FP/sum(n.0),
           M=M)
  grande=rbind(grande,performance.log2)
}

## Averaged out the thresholds, FPRs, and TPRs for a given estimate point

temp=grande %>% group_by(N) %>% summarize(TPR=mean(TPR), FPR=mean(FPR), p.hat=mean(p.hat))
```

```{r}

## ROC of M simulations

library(yardstick)

grande %>%
  ggplot(aes(FPR, TPR, group=M)) + geom_line(color="blue") +
  geom_abline(lty=3) +
  coord_equal()

```


:::

    

## e. Contest Part 1: Predict the estimated *probability* of linkage for the test data (using any model). 
- Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 
- You are free to use any tuning parameters
- You are free to use any data transformation or feature engineering
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
- Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric)
$$ 
L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$
where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels.

::: {.solution}

```{r}

## Going to use a logistic regression with lasso penalty (to remove variables). Using lambda min. I'm going to remove TIMERANGE (dont really see relevance and has small impact among models. Will likely use lambda min.

M=1
K=10 # number of folds
grande=tibble()

## Training and test set preparation

df.train3 = df.train %>% select(-TIMERANGE)
df.test3 = df.test %>% select(-TIMERANGE)
n.train=length(df.train3$y)
  
  ## Setup for linear regression model with lasso penalty
  
  X=glmnet::makeX(select(df.train3,-y),df.test3)
  train.x=X$x
  train.y=df.train3$y
  test.x=X$xtest
  
## Set up fold indices

folds=rep(1:K, length=nrow(train.x)) %>% sample()
  
#### Choose optimal alpha that minimizes the loss function
  
## Returns minimum log loss of a given alpha (given lambda min)
  
est_meanLL=function(alpha,folds){
  
  fit.log3=cv.glmnet(train.x, train.y, alpha=1, foldid=folds, family="binomial") # Finding minimal lambda that minimizes binomial deviance (neg log loss function)
  min(fit.log3$cvm)
}

## Set alpha sequence
alpha.seq = seq(0,1,by=0.05)

##  cross validation to find the alpha with the lowest log loss

alpha_loss = tibble()
for(i in 1:length(alpha.seq)){
  r1 = tibble(alpha = alpha.seq[i],
              min_LL=est_meanLL(alpha.seq[i],folds))
  alpha_loss=rbind(alpha_loss, r1)
}
alpha.hat=alpha_loss$alpha[21]
alpha.hat

## Choose optimal lambda given alpha

fit.log3=cv.glmnet(train.x, train.y, alpha=alpha.hat, foldid=folds, family="binomial")
plot(fit.log3)
lambda.hat=fit.log3$lambda.min

## Make predictions

p.hat = predict(fit.log3, test.x, type="response",s="lambda.min")

```

```{r echo=FALSE}
submission=tibble(p=p.hat)
write.csv(submission,"/Users/nkeeley/Dropbox/SY_Q1/Statistical Learning/keeley_nicholas_1.csv", row.names = TRUE)
```


:::




## f. Contest Part 2: Predict the linkages for the test data (using any model). 
- Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **linkage** that takes the value of 1 for linkages and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact. 
- You are free to use any tuning parameters.
- You are free to use any data transformation or feature engineering.
- Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Positives (FP)    
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests. 

::: {.solution}

```{r}

## Create a confusion matrix for TRAINING data

df.train3 = df.train %>% select(-TIMERANGE)
df.test3 = df.test %>% select(-TIMERANGE)
n.train=length(df.train3$y)
h=sample(1:n.train,(.1*n.train))
df.holdout3=df.train3[h,]
df.train3=slice(df.train3,-h)


## Setup for linear regression model with lasso penalty
  
X=glmnet::makeX(select(df.train3,-y), select(df.holdout3,-y))
  train.x=X$x
  train.y=df.train3$y
  test.x=X$xtest

## Reuse the model from part 1 and predict probs for holdout

fit.log3=cv.glmnet(train.x, train.y, alpha=alpha.hat, foldid=folds, family="binomial")
p.hat = predict(fit.log3, test.x, type="response", s="lambda.min")

## Confusion matrix optimization to find the optimal hard classification cutoff

measure = tibble()
inc=0.005
thresh.seq=seq(0,1,by=inc)
length(thresh.seq)
  G.test=df.holdout3$y
  
for(i in 1:length(thresh.seq)){
  G.hat=ifelse(p.hat >= thresh.seq[i],1,0)
  confuse=table(predicted=G.hat, truth = G.test) %>% addmargins()
  TN = confuse[1,1]
  FP = confuse[2,1]
  FN = confuse[1,2]
  TP = confuse[2,2]
  cost=FP+(8*FN)
  temp=tibble(thresh=thresh.seq[i],
              cost=cost)
  measure = rbind(measure, temp)
}

temp=min(measure$cost)
temp
answer=measure$thresh[(measure$cost==temp)]
answer

## Fit the new model with test data using the optimal threshold

df.train3 = df.train %>% select(-TIMERANGE)
df.test3 = df.test %>% select(-TIMERANGE)

X=glmnet::makeX(select(df.train3,-y), df.test3)
  train.x=X$x
  train.y=df.train3$y
  test.x=X$xtest
  
## Model fit

fit.log3=cv.glmnet(train.x, train.y, alpha=alpha.hat, foldid=folds, family="binomial")
p.hat = predict(fit.log3, test.x, type="response", s="lambda.min")

## Apply the threshold

G.hat=ifelse(p.hat >= answer,1,0)
submit = tibble(linkage=G.hat)
```

```{r echo=FALSE}

## Save estimates as CSV

write.csv(submit,"/Users/nkeeley/Dropbox/SY_Q1/Statistical Learning/keeley_nicholas_2.csv", row.names = TRUE)


```

:::
